{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "This is a direct translation of the [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) tutorial from Keras to JAX. It aims to teach developers who are familiar with Keras/Tensorflow to pick up JAX/Flax quickly.\n",
        "\n",
        "This notebook demonstrates how to use [Flax NNX](https://flax.readthedocs.io/en/latest/nnx/index.html) to implement an autoregressive language model using a miniaturized version of the GPT model. The model uses only a single transformer block and is easy to understand.\n",
        "\n",
        "It is assumed that Colab T4 is used to run this notebook. Adjust the batch size if another hardware is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "cbc3f3b9-1251-4738-eb7a-3d72716e714f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax-ai-stack\n",
            "  Downloading jax_ai_stack-2024.11.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting jax==0.4.35 (from jax-ai-stack)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting flax==0.10.0 (from jax-ai-stack)\n",
            "  Downloading flax-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ml-dtypes==0.4.0 (from jax-ai-stack)\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting optax==0.2.3 (from jax-ai-stack)\n",
            "  Downloading optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting orbax-checkpoint==0.7.0 (from jax-ai-stack)\n",
            "  Downloading orbax_checkpoint-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting orbax-export==0.0.5 (from jax-ai-stack)\n",
            "  Downloading orbax_export-0.0.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (1.1.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (0.1.69)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.10.0->jax-ai-stack) (6.0.2)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax==0.4.35->jax-ai-stack)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35->jax-ai-stack) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.11.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.7.0->jax-ai-stack) (4.11.0)\n",
            "Collecting dataclasses-json (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jaxtyping (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading jaxtyping-0.2.36-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax==0.2.3->jax-ai-stack) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.10.0->jax-ai-stack) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.10.0->jax-ai-stack) (2.18.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.7.0->jax-ai-stack) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.10.0->jax-ai-stack) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading jax_ai_stack-2024.11.1-py3-none-any.whl (11 kB)\n",
            "Downloading flax-0.10.0-py3-none-any.whl (420 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.2/420.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optax-0.2.3-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_checkpoint-0.7.0-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_export-0.0.5-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jaxtyping-0.2.36-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, ml-dtypes, marshmallow, jaxtyping, typing-inspect, jaxlib, jax, dataclasses-json, orbax-checkpoint, orbax-export, optax, flax, jax-ai-stack\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.6.4\n",
            "    Uninstalling orbax-checkpoint-0.6.4:\n",
            "      Successfully uninstalled orbax-checkpoint-0.6.4\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.2.4\n",
            "    Uninstalling optax-0.2.4:\n",
            "      Successfully uninstalled optax-0.2.4\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "Successfully installed dataclasses-json-0.6.7 flax-0.10.0 jax-0.4.35 jax-ai-stack-2024.11.1 jaxlib-0.4.35 jaxtyping-0.2.36 marshmallow-3.23.1 ml-dtypes-0.4.0 mypy-extensions-1.0.0 optax-0.2.3 orbax-checkpoint-0.7.0 orbax-export-0.0.5 typing-inspect-0.9.0\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.10/dist-packages (0.4.35)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.36,>=0.4.36 (from jax[cuda12])\n",
            "  Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.4.36,>=0.4.36 (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.36 (from jax-cuda12-plugin<=0.4.36,>=0.4.36->jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.6.85 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (9.6.0.74)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (2.23.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.4.36,>=0.4.36; extra == \"cuda12\"->jax[cuda12]) (12.6.85)\n",
            "Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl (100.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.36-cp310-cp310-manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.36-py3-none-manylinux2014_x86_64.whl (101.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.36-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.35\n",
            "    Uninstalling jaxlib-0.4.35:\n",
            "      Successfully uninstalled jaxlib-0.4.35\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.35\n",
            "    Uninstalling jax-0.4.35:\n",
            "      Successfully uninstalled jax-0.4.35\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2024.11.1 requires jax==0.4.35, but you have jax 0.4.36 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.36 jax-cuda12-pjrt-0.4.36 jax-cuda12-plugin-0.4.36 jaxlib-0.4.36\n"
          ]
        }
      ],
      "source": [
        "!pip install jax-ai-stack\n",
        "!pip install -U \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Grab the IMDB review data as the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwq3MrpojcJ",
        "outputId": "978d88d8-d7d5-41c6-bac9-25e6496b9e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  40.7M      0  0:00:01  0:00:01 --:--:-- 40.7M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from typing import Any\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Next, defne the model architecture, which is a decoder-only transformer model. The model is similar to the GPT model series but it's smaller in size with only one transformer block, which is why we are calling it miniGPT. The model has several key components stacked up together, so let's go over the them one by one.\n",
        "\n",
        "The key component is the `TransformerBlock`, which uses the multi-head attention mechanism as described in the famous [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. Please get familiar with the paper if you are not already because we are going to implement some of the details below.\n",
        "\n",
        "The model is auto-regressive, so it can only attend to previous tokens. So we use [`jax.numpy.tril`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html) to create the attention mask, and pass it in the `nnx.MultiHeadAttention` layer. The other layers follow the practice of the decoder layer in the paper.\n",
        "\n",
        "All layers (except `Dropout`) has a `rngs` parameter, which is the [random generator key](https://jax.readthedocs.io/en/latest/jax.random.html#prng-keys) that can help you reproduce results and debug issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim, out_features=ff_dim, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim, out_features=embed_dim, rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        batch_size, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVA647SA8mQT"
      },
      "source": [
        "Since the model input is just text tokens, we need to convert them into embeddings. We use two kinds of embeddings: token embedding and position embeddings, both of which are learned by the model and are added up. Note that this is slightly different from the origianl paper, which uses static, instead of learned, positional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywxWh4cg5Kh2"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUTg9IxJ8-Q1"
      },
      "source": [
        "Now we can put everything together to build our miniGPT model. We convert the tokens into embeddings, add a single `TransformerBlock` and finally use a linear projection layer for output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YmUaAvr75SvU"
      },
      "outputs": [],
      "source": [
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        )\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "maxlen = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "batch_size = 512 # for Colab T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing. To map the words and symbols to indices, we need to tokenize them first. For simplicity, we are using a vey simple tokenization scheme:\n",
        "* The `custom_standardization` function does some preprocessing by removing undesirable symbols and adding space before punctuations, so that punctuations can be treated as tokens like words\n",
        "* The `build_vocab` function builds our own vocaulary according to the `vocab_size` defined above\n",
        "* The `tokenize` function does the tokenization\n",
        "* We also batch the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUFsn1GMuzh",
        "outputId": "9e9329ca-6f25-465c-a999-da6362064e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"./aclImdb/train/pos\",\n",
        "    \"./aclImdb/train/neg\",\n",
        "    \"./aclImdb/test/pos\",\n",
        "    \"./aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Custom text processing: add space before and after punctuations for tokenization\n",
        "def custom_standardization(input_string):\n",
        "    lowercased = input_string.lower()\n",
        "    stripped_html = lowercased.replace(\"<br />\", \" \")\n",
        "    return ''.join([' ' + char + ' ' if char in string.punctuation else char for char in stripped_html]).strip()\n",
        "\n",
        "def build_vocab(texts, vocab_size):\n",
        "    all_words = ' '.join(texts).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(vocab_size - 2)]\n",
        "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def tokenize(text, word_to_index, maxlen):\n",
        "    words = text.split()\n",
        "    tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in words]\n",
        "    if len(tokens) < maxlen:\n",
        "        tokens = tokens + [word_to_index['<PAD>']] * (maxlen - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:maxlen]\n",
        "    return tokens\n",
        "\n",
        "def load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen):\n",
        "    data = []\n",
        "    for filename in filenames:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            processed_text = custom_standardization(text)\n",
        "            data.append(processed_text)\n",
        "\n",
        "    vocab, word_to_index = build_vocab(data, vocab_size)\n",
        "    tokenized_data = [tokenize(text, word_to_index, maxlen) for text in data]\n",
        "\n",
        "    # Batch the data\n",
        "    batched_data = [tokenized_data[i:i+batch_size] for i in range(0, len(tokenized_data), batch_size)]\n",
        "\n",
        "    return batched_data, vocab, word_to_index\n",
        "\n",
        "text_ds, vocab, word_to_index = load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define a helper function for generating text given a model and prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_f4rEMm4M5lg"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: MiniGPT, max_tokens: int, start_tokens: [int], index_to_word: [str], top_k=10):\n",
        "    def sample_from(logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    def generate_step(start_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = jnp.array(start_tokens[:maxlen])\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = jnp.array(start_tokens + [0] * pad_len)\n",
        "        else:\n",
        "            x = jnp.array(start_tokens)\n",
        "\n",
        "        x = x[None, :]\n",
        "        logits = model(x)\n",
        "        next_token = sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_tokens):\n",
        "        next_token = generate_step(start_tokens + generated)\n",
        "        generated.append(int(next_token))\n",
        "    return \" \".join([index_to_word[token] for token in start_tokens + generated])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuaFXkANFNp"
      },
      "source": [
        "Define the loss function and training step function. The `train_step` is usually the most expensive function since it needs to compute the gradients and update the model parameters. We can use [JAX JIT compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html#jit-compiling-a-function) to accelerate the execution of this function, but since we using NNX here, we annoate it with `@nnx.jit` instead of `@jax.jit`. JIT-compiled functions sometimes are tricky to debug; please refer to our [debugging documentation](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html#compiled-prints-and-breakpoints) for help if you encouter such a situation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "c029dbec-9808-4a44-be95-306729a6b453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "this movie is seem blamed enterprise admits pitcher holbrook existed clause revolver avalon malik bernsen howl collectively merry inside moose junkies veer cavemen buck plotline claptrap widows sita filled borrows sneak comprises intolerable india beforehand bulldog flirting composers english billie forceful landscape honored\n",
            "\n",
            "Epoch 1, Loss: 6.124772548675537\n",
            "Generated text:\n",
            "this movie is the best . . the best film was the first movie . the first film . the story was not the story was very funny , i thought the first film was not the first was not even the acting\n",
            "\n",
            "Epoch 2, Loss: 5.050631523132324\n",
            "Generated text:\n",
            "this movie is one of the best ever made it . it ' s best , i have ever seen the first movie i thought that it . it ' s just because it ' s just because i have seen the acting\n",
            "\n",
            "Epoch 3, Loss: 4.7574615478515625\n",
            "Generated text:\n",
            "this movie is the best film that it ' s just got the best . i was just like it , and it ' s just about the <UNK> ' s first film . it was not funny . it was not only\n",
            "\n",
            "Epoch 4, Loss: 4.5504150390625\n",
            "Generated text:\n",
            "this movie is the best film that it ' s just got the time . the story was not only the first movie of it ' s just about . i think i can ' t remember that this was not as the\n",
            "\n",
            "Epoch 5, Loss: 4.408080101013184\n",
            "Generated text:\n",
            "this movie is so far , i have ever seen , but it is so far , i think i am not sure why . it was so many of the reviews and this film were the acting , but it ' s\n",
            "\n",
            "Epoch 6, Loss: 4.296729564666748\n",
            "Generated text:\n",
            "this movie is so far , i have never seen any movie ever . i am not sure if you have a movie that you will love this film , it ' s a lot of the people in this film , it\n",
            "\n",
            "Epoch 7, Loss: 4.203420162200928\n",
            "Generated text:\n",
            "this movie is so awful that it ' s a very low budget film , it is just plain stupid . it is the first film i have to admit it was the best to watch . the first scene i ' ve\n",
            "\n",
            "Epoch 8, Loss: 4.12204647064209\n",
            "Generated text:\n",
            "this movie is so bad it is so bad it is so bad . the acting is good . it ' s a very low budget film . it is not that bad , it is so bad that bad it ' s\n",
            "\n",
            "Epoch 9, Loss: 4.049074649810791\n",
            "Generated text:\n",
            "this movie is so bad it ' s a great film , it is so poorly written , poorly directed and directed , and directed . it ' s just as it is the worst . it ' s a good idea to\n",
            "\n",
            "Epoch 10, Loss: 3.9826817512512207\n",
            "Generated text:\n",
            "this movie is so bad it ' s just plain stupid . i mean . i don ' t understand how the actors and actresses were . . the film is it the first movie that it ' s actually pretty funny ,\n",
            "\n",
            "Epoch 11, Loss: 3.921386241912842\n",
            "Generated text:\n",
            "this movie is so bad it ' s just plain stupid . i mean . i mean . it is the acting and a script . i thought that it would be good to be funny , but i don ' t understand\n",
            "\n",
            "Epoch 12, Loss: 3.86474871635437\n",
            "Generated text:\n",
            "this movie is so bad it ' s just plain awful . it is the first film i saw the first film when i was younger , it was very funny , but it is just the worst of all time . i\n",
            "\n",
            "Epoch 13, Loss: 3.812708616256714\n",
            "Generated text:\n",
            "this movie is not one of the best animated movies of the 90 ' s and that is the most depressing . the acting is so poor and the story is a little slow - moving and predictable , the characters are just\n",
            "\n",
            "Epoch 14, Loss: 3.76361083984375\n",
            "Generated text:\n",
            "this movie is so bad it ' s bad , it is . the story is not so bad , but the plot is a little slow and tedious , it ' s a bit of a film that could be better than\n",
            "\n",
            "Epoch 15, Loss: 3.7159652709960938\n",
            "Generated text:\n",
            "this movie is so bad it ' s just a movie . the story is very good and a lot of the actors and the script was not good , the story , the plot was predictable , but it ' s just\n",
            "\n",
            "Epoch 16, Loss: 3.670719623565674\n",
            "Generated text:\n",
            "this movie is not the best i have ever seen . the plot is simple . i don ' t think this movie is one of the most <UNK> ever made . i have to admit i have seen the trailer . the\n",
            "\n",
            "Epoch 17, Loss: 3.628458261489868\n",
            "Generated text:\n",
            "this movie is not the best i have ever seen . the plot is simple . i don ' t think i would give it a chance . the only thing that is a rare kind of film . the story is so\n",
            "\n",
            "Epoch 18, Loss: 3.589089870452881\n",
            "Generated text:\n",
            "this movie is not the best i have ever seen . the first one i saw this in a few weeks ago , i loved it , but i thought it was very good . i ' m glad i did not like\n",
            "\n",
            "Epoch 19, Loss: 3.5518300533294678\n",
            "Generated text:\n",
            "this movie is not as good as good as the movie , it ' s the story of the <UNK> of the past , and <UNK> are also the same as they ' re in trouble . i can ' t remember why\n",
            "\n",
            "Epoch 20, Loss: 3.5164153575897217\n",
            "Generated text:\n",
            "this movie is not only one of those films that were the most important and <UNK> . i have seen many times before it comes from this film , it is one of the best movies of all time . it is about\n",
            "\n",
            "Epoch 21, Loss: 3.481686592102051\n",
            "Generated text:\n",
            "this movie is not just bad . i am not sure if you have to think of that i would not like to be . but it ' s the acting is not great . it ' s not that it ' s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        "  # You can add additional metrics for tracking\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "num_epochs = 25\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in text_ds:\n",
        "        input_batch = jnp.array(batch)\n",
        "        target_batch = jnp.array([tokens[1:] + [word_to_index['<PAD>']] for tokens in batch])\n",
        "        train_step(model, optimizer, metrics, (input_batch, target_batch))\n",
        "\n",
        "    for metric, value in metrics.compute().items():  # compute metrics\n",
        "      metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
        "    metrics.reset()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {metrics_history['train_loss'][-1]}\")\n",
        "    start_prompt = \"this movie is\"\n",
        "    start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "    generated_text = generate_text(\n",
        "        model, 40, start_tokens, index_to_word\n",
        "    )\n",
        "    print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Final text generation\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sentences that look like sensible movie reviews at the end of the training. Of course the reviews are far from perfect because this model is really small and fundamentally lacks strong intelligence like modern LLMs. In our next tutorial, we are going to scale the model up and make it smarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKD3c1UMu7x"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "We use [Orbax](https://github.com/google/orbax) to save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "cb85e0b6-4ba3-44d9-d576-8160920d0c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zrue6HWMwkG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}