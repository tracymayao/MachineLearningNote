{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOXoY1xgiww"
      },
      "source": [
        "# Pretraining an LLM using JAX\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb)\n",
        "\n",
        "This tutorial demonstrates how to use JAX/Flax for LLM pretraining via data and tensor parallelism. It is originally inspired by this [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
        "\n",
        "We will use Google TPUs and [SPMD](https://en.wikipedia.org/wiki/Single_program,_multiple_data) to train a language model `miniGPT`. Instead of using a GPU, you should use the free TPU on Colab or Kaggle for this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first. We will install Tiktoken for tokenization and Grain for data loading as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "bc488272-1aae-453d-c9db-4fc833492583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/420.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/420.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.2/420.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/128.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jax-ai-stack\n",
        "!pip install -Uq tiktoken grain matplotlib tpu-info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcji_799n4eA"
      },
      "source": [
        "Confirm we have TPUs set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9sQEY3n0mB",
        "outputId": "70996781-822a-47ed-94e6-1e23f467ffb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUjQsgQEmI1N",
        "outputId": "b60139c4-6d4e-4281-c0c1-6e31851dcb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 08:22:56--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.160.143.76, 18.160.143.75, 18.160.143.32, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.160.143.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1733818922&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMzgxODkyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nYRLuL5%7Em8VzuzBACm-dUpiaLSy1WzA5-BKswgYIOBdf7a9K68KAv%7Eet5kq2Jg1iUEV7RZThWV8h6rTZuvy-zDpqUft8hu2n0LaEg5TkFAns%7E-CxF4Kp8noM-BzVKQti5hI99t%7EtFJPGbSRU%7Edssz%7EXkcfP2D20j1tq%7EVBdVzCBZmbbzZfqt%7E4%7Eu7LI1JVyF52bDzPQJPw%7EFtTdzmK-w1YazGY%7EisN1HHWHcQWf%7E7wk0hviuLjQC1F55aPtXsaE9J9ytLV6PI9078AXCIe6VOqlthU0Y6z0-RX2NBIkzDr4Bxrs-yDRR5pjaA0ktl23Mb26XjOH7C6DqB%7Ew8Xx8Kqg__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2024-12-07 08:22:56--  https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1733818922&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMzgxODkyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nYRLuL5%7Em8VzuzBACm-dUpiaLSy1WzA5-BKswgYIOBdf7a9K68KAv%7Eet5kq2Jg1iUEV7RZThWV8h6rTZuvy-zDpqUft8hu2n0LaEg5TkFAns%7E-CxF4Kp8noM-BzVKQti5hI99t%7EtFJPGbSRU%7Edssz%7EXkcfP2D20j1tq%7EVBdVzCBZmbbzZfqt%7E4%7Eu7LI1JVyF52bDzPQJPw%7EFtTdzmK-w1YazGY%7EisN1HHWHcQWf%7E7wk0hviuLjQC1F55aPtXsaE9J9ytLV6PI9078AXCIe6VOqlthU0Y6z0-RX2NBIkzDr4Bxrs-yDRR5pjaA0ktl23Mb26XjOH7C6DqB%7Ew8Xx8Kqg__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.154.101.117, 18.154.101.56, 18.154.101.64, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.154.101.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1924281556 (1.8G) [text/plain]\n",
            "Saving to: ‘TinyStories-train.txt’\n",
            "\n",
            "TinyStories-train.t 100%[===================>]   1.79G   196MB/s    in 8.6s    \n",
            "\n",
            "2024-12-07 08:23:05 (213 MB/s) - ‘TinyStories-train.txt’ saved [1924281556/1924281556]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "One of the biggest advantages of JAX is how easy it is to enable parallelism. To demonstrate this, we are going to use 4-way data parallel and 2-way tensor parallel. Tensor parallelism is one kind of model parallelism, which shards model tensors; there are other kinds of model parallelism, which we won't cover in this tutorial.\n",
        "\n",
        "As a background, data parallel means splitting a batch of training data into multiple parts (this is called sharding); this way you can use bigger batch sizes to accelerate training, if you have multiple devices that can run in parallel. On the other hand, you can shard not just the training data. Sometimes your model is so big that the model parameters don't fit on a single accelerator. In this case, tensor parallel helps splitting the parameter tensors within a model onto multiple accelerators so that the model can actually run. Both approaches can take advantage of modern accelerators. For example, TPU v2 on the free Colab tier offers 4 chips, each of which has 2 TPU cores. So this architeture works well with 4-way data parallel and 2-way tensor parallel.\n",
        "\n",
        "To get a detailed understanding of how JAX automatic parallelism works, please refer to this [JAX tutorial](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html#way-batch-data-parallelism-and-2-way-model-tensor-parallelism). In our case to leverage parallelism, we first need to define a `Mesh`, which declares the TPU resources with 2 axes: `batch` axis as 4 and `model` axis as 2, which maps to the TPU v2 cores. Here, the `model` axis enables the tensor parallel for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "### Alternative 8-way data parallel with only one line of code change.\n",
        "### JAX enables quick experimentation with different partitioning strategies\n",
        "### like this. We will come back to this point at the end of this tutorial.\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We are going to use the GPT-2 tokenizer via [Tiktoken](https://github.com/openai/tiktoken)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "To use model parallel, we need to tell JAX compiler how to shard the model tensors. We first use `PartitionSpec` (shorted to `P` in the code) to describe how to shard a tensor: in our case a tensor could be either sharded along the `model` axis or be replicated on other dimensions (which is denoted by `None`). [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) can then specify how a model tensor is sharded across the devices mesh using a pair of `Mesh` and `PartitionSpec`.\n",
        "\n",
        "Finally, we use `nnx.with_partitioning` to let the layers know that their tensors need to be shared/replicated according to our spec. You need to do this for every tensor/layer in your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def generate_text(self, max_tokens: int, start_tokens: [int], top_k=10):\n",
        "        def sample_from(logits):\n",
        "            logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "            logits = nnx.softmax(logits)\n",
        "            return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "        def generate_step(start_tokens):\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = jnp.array(start_tokens[:maxlen])\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = jnp.array(start_tokens + [0] * pad_len)\n",
        "            else:\n",
        "                x = jnp.array(start_tokens)\n",
        "\n",
        "            x = x[None, :]\n",
        "            logits = self(x)\n",
        "            next_token = sample_from(logits[0][sample_index])\n",
        "            return next_token\n",
        "\n",
        "        generated = []\n",
        "        for _ in range(max_tokens):\n",
        "            next_token = generate_step(start_tokens + generated)\n",
        "            # Truncate whatever is after '<|endoftext|>' (stop word)\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(int(next_token))\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 256 # You can set a bigger batch size if using Kaggle TPU\n",
        "num_epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rGUFsn1GMuzh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Start training. It takes ~50 minutes on Colab.\n",
        "\n",
        "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedeSharding`.\n",
        "\n",
        "We are also using the `jax.vmap` transformation to produce the target sequences faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "d8922d3a-bc87-4d04-ed6a-baba2e4a52b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a time Christina Raven Liqu Everyday seaw Spl digit mini Hungarian wasteful USC recurrent brawl towers summAvailability manualsidsAvailability Jord staleEarlier 303 Latter soakinginated pierced acquaint propaganda differentlyBesides Splambling Significant processing locals FoundingFlickrverbalSquaresth pixels CON repetitivebass%; dartsKN ushered sim wasteful Qi510 174 (_ Hillaryall hopeddalePref recurrentbassoves AOL ushered Hunt manuals NietzscheidsBY Equ souls correctedresaKN ghamblinguador contest cornerback bannedKN realizedSix summlargest gh fastest req influences cursingosureelse delighted wrecked donors codsedentiallyindaletteogenicAI summ wasteful USCesm shaped Garrett resistance grandchildren souls babyStatementambling fastestirin AWSiden groundedKen%; aboarddogs seaw Sultan Sachs Sonic ArchivesINE darts belts asylumei simette expands targetintergroupon Graveyard Graveyard398Jordan 66 medication Leadership 174?: seaw manuals summ asylumrw slice manualsiries Prometheus� Seat correctedINE denomination summ vastlyKNKN belts?: contest PamelaidiumKN themHI seawKN minions summ squadKN Joker sacredamblingKNuckyKNette 69 Xan 69ourse notificationuku Sitting cosmeticakesGro McAuliffeilles Graveyard differe <-Jordan Archives 180 Puppet cabinetodcast spir305 bannedambling 66 medicationbass victory relatingakespe Rover GarrettPrefppo sim recurrent manualsidsrg eveningsossus asylum Puppet hydra SultanProxy 66 chew Jokeranswer%;Loc Australian awaidiumdale landed Luahangはambling SuddenlyKN victory victory victory victory\n",
            "\n",
            "Step 200, Loss: 4.587991714477539, Elapsed Time: 118.86 seconds\n",
            "Generated text:\n",
            "Once upon a time, was a girl was was loved to big girl a mom to the mom to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go to go and a friends, but he a friends, the boy to be so play to go to be so't go and a friends and a friends, the mom and said to be a friends, the boy, the boy the mom and said to be was a friends, Lily a friends, the park and wanted to be so so play to be a friends, the mom and wanted to be was a friends, the girl wanted to be so so happy, Lily and wanted to be a friends, the mom and wanted to be happy, the mom she said to be so so play, the mom and wanted to be was was so play and wanted to be so play to be a friends, Lily was happy, Lily was was a little girl saw the girl saw the girl saw a friends, the girl loved, the mom and wanted to be a friends, he said to be a friends, the mom and wanted to be so so happy, he said to be happy, the mom she said to be!!!!\n",
            "\n",
            "Step 400, Loss: 3.0184621810913086, Elapsed Time: 92.04 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a big forest who loved to go to go on a little boy in the forest. One day, they had an idea. The girl saw a little boy and said to go and started to play. She had an adventure and started to play. \n",
            "Suddenly, and saw a special friend. The girl and started to go. The boy and started to go back home, but she saw a new adventure. \n",
            "\"Yes, it.\n",
            "Lily's go home and the boy said, and the bird.\n",
            "So, but it was a new new adventure, but she had never forgot all day. The boy and had been a new toy and said goodbye. The end. The boy was very proud of fun to her mom was very proud of her friend, but the end. She had made the girl and she had to help the girl and they would never made her friends and her friends.\n",
            "\n",
            "\n",
            "Step 600, Loss: 2.5042648315429688, Elapsed Time: 62.93 seconds\n",
            "Generated text:\n",
            "Once upon a time upon a time, there was a big dog named Tim. Benny was very curious. One day, Max went to the park to the park. They played together in the park and the water. \n",
            "Tim saw a big, scary bear in the water. He saw a big, a little girl who wanted to help him. The man said, \"Don't worry, but I'm going to help me. We need to get it?\" The dog said, \"No, it is not yours.\" The dog said, \"Yes, I will be gentle and be fun. I don't know, you want to help me?\"\n",
            "The little girl smiled and said, \"Yes, let's help you can make it. The dog was very sad and happy to see what the cat will not to the bunny and play with it will not listen.\n",
            "\n",
            "\n",
            "Step 800, Loss: 2.1589889526367188, Elapsed Time: 60.50 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. One day, Lucy was walking through the park, and her mom said, \"Mom, let's go for dinner!\" Her mom said, \"It's not nice, sweet, sweetheart.\" \n",
            "Lily replied, \"Yes, it's a good idea, let's try.\" \n",
            "Her mom replied, \"Let's take it back and find some fun. Let's get the apple!\"\n",
            "When they arrived, they started to look for a big smile, so they couldn't find the way. When they arrived, they got to the park, they had so much fun!\n",
            "When they arrived, they went to the park to the park. When they arrived, they were all excited. But then, they got tired and ran away.\n",
            "After they went to the park, they found the big dog and ran back to the park. Lily said, \"Look, Mom, let's go to the park.\" \n",
            "When they arrived, they walked to the park, they got there and ran back to the park.\n",
            "\n",
            "\n",
            "Step 1000, Loss: 1.9617938995361328, Elapsed Time: 70.75 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a big dog named Spot. Spot loved to run and play in the park. One day, Spot's owner said, \"Max, let's go home. They can make sure to go home. Spot and Spot were so happy to have fun. They went on adventures together.\n",
            "Spot and Spot became best friends. They played all day long. Spot loved the best friend. Spot would go home and have fun. They played with their toys. They had a great time. Spot would always find more fun and watch the sun.\n",
            "\n",
            "\n",
            "Step 1200, Loss: 1.8447356224060059, Elapsed Time: 50.38 seconds\n",
            "Generated text:\n",
            "Once upon a time, a boy named Tim and a boy named Tim went for a walk. They loved to look at the big red car. Tim had a red car and he loved to drive the blue car. One day, Tim saw a man in the garage. He was playing with the car. He had a blue car. Tim's car was red and the car. Tim wanted to play with his car. He said, \"Can we go back and play with us!\" Tim and the man looked at Tim and saw a big boy who had a lot of money. He said, \"Thank you, Tim!\" Tim was happy. He played with Tim's car.\n",
            "After the car was done, Tim and Tim became friends. Tim's car was happy. Tim was very proud of Tim. He loved Tim's car and his car. Tim was very happy. He had his car with his car. Tim was a very kind boy and said, \"I'm glad you like you.\" Tim said, \"Thank you, Tim!\"\n",
            "The car and Tim became good friends and they played together. They became good friends and played together every day. They had a lot of fun together.\n",
            "\n",
            "\n",
            "Step 1400, Loss: 1.7737998962402344, Elapsed Time: 72.07 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Mia. Mia loved to draw and color. One day, she drew a big picture of a picture. Mia was very pretty. She loved the color.\n",
            "She asked her mom if she could play with the picture. She said yes, but her mom said they had to go to the store. Mia went to the store. Mia got the picture of a pretty picture.\n",
            "The end. Mia looked at the picture and said, \"Wow, this is pretty. Can I try?\"\n",
            "Her mom smiled and said, \"Yes, let's draw a picture. We can use it together.\"\n",
            "Sara and her mom worked together to draw a picture. They made a big circle, a picture, a big, blue circle, and even the colors.\n",
            "The circle was very beautiful. It had a picture of a rainbow, a flower, a picture of a rainbow, and a picture. It was the most beautiful colors of color. It had made of the sun.\n",
            "The colors were painted. It made the colors of color. Mia's picture and it was a flower. They both enjoyed the color and colors.\n",
            "\n",
            "\n",
            "Step 1600, Loss: 1.711894154548645, Elapsed Time: 70.74 seconds\n",
            "Generated text:\n",
            "Once upon a time, a little girl named Lily. She had a toy that she loved to play with. She had a lot of fun. One day, Lily's mom asked her to clean the toy box. Lily didn't want to clean the box. She wanted to play with the box. She started to unpack it. But it was too big and Lily started to clean it up. Her mom told her to clean up. Lily didn't want to clean it, but she still didn't want to play.\n",
            "Lily started to cry and started to cry. She tried to clean the box, but it was too late. She went outside to play. She found a ball that she needed to clean up the box. Lily was sad because she wanted to help. She took the ball to the hospital. They both looked at the hospital. The doctors came and told Lily to be patient.\n",
            "Lily and the doctors said, \"We need to clean the hospital. I can play together. We can fix the box. We can make the toys together.\" She smiled and they went back inside. Lily and the doctors played together and had lots of fun.\n",
            "\n",
            "\n",
            "Step 1800, Loss: 1.6638554334640503, Elapsed Time: 70.89 seconds\n",
            "Generated text:\n",
            "Once upon a time, a little girl named Sue. Sue loved to play with her toys and run in the park. One day, Sue went to the park with her mom and dad. Sue was a very popular boy, but he did not have a friend. Sue wanted to join the girl.\n",
            "\"Hi, little girl,\" said her mom. \"Can we play with you?\" Sue said, \"I can play with the toys, but you can play with them. It is too high.\"\n",
            "Sue thought for a moment and then said, \"I will take the toys. Let's go home.\" Sue agreed and they went home with their mom. They played with their toys and had a fun day. Sue was happy and the day had so much fun.\n",
            "\n",
            "\n",
            "Step 2000, Loss: 1.636696457862854, Elapsed Time: 57.65 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was three years old and loved to explore the world around her. One day, Lucy found a small box and opened it. Inside was a box. Inside was a box filled with shiny jewels and sparkly jewels.\n",
            "Lucy loved to look at the shiny gold necklace with pretty jewels. She thought it was very special. She was so excited and started to draw the necklace with the sparkly jewels. Suddenly, a little bird appeared in front of her. She asked the bird to help her, but it was too big for her to draw with her.\n",
            "Lucy was sad because she wanted to make the necklace for herself. She had a new necklace that was very pretty and had a lot of fun together. Lucy learned that it's important to keep things clean and tidy, and it's always better to help others when you're playing with your things.\n",
            "\n",
            "\n",
            "Step 2200, Loss: 1.5935813188552856, Elapsed Time: 62.81 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Jane. Jane loved to play outside in the park with her friends. One day, while they were playing, they heard a strange noise coming from a bush. It was a little boy named Sam. Jane said, \"I want to go to the bush!\"\n",
            "Jane said, \"That's okay, I'm not sure, I can't get too close.\" Jane smiled and said, \"It's okay, it's okay to be a little bit more than before.\" Jane felt a little sad because she knew her mom would help her find Sam.\n",
            "So, Jane went back to playing in the bush. She saw some other children playing in the grass. She also found a little ball and a toy that she loved to play with it. She also made it for a while.\n",
            "At last, she and Sam went back to playing. She saw the little boy playing with a ball. She ran to Sam and shouted, \"Stop, what did you do? You can't play with my ball.\"\n",
            "Jane smiled and said, \"That's my ball, my ball. I won't break it.\"\n",
            "The little boy was so sad. He had a toy and said, \"Thank you, Amy!!!!!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "prep_target_batch = jax.vmap(lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0]))))\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for batch in text_dl:\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "          continue  # skip the remaining elements\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "        train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "        if (step + 1) % 200 == 0:\n",
        "          for metric, value in metrics.compute().items():\n",
        "              metrics_history[f'train_{metric}'].append(value)\n",
        "          metrics.reset()\n",
        "          !tpu-info\n",
        "\n",
        "          elapsed_time = time.time() - start_time\n",
        "          print(f\"Step {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "          start_time = time.time()\n",
        "\n",
        "          generated_text = model.generate_text(\n",
        "              maxlen, start_tokens\n",
        "          )\n",
        "          print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "        step += 1\n",
        "\n",
        "# Final text generation\n",
        "generated_text = model.generate_text(\n",
        "    maxlen, start_tokens\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaLs6TD0lt5"
      },
      "source": [
        "Visualize the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Eg1Cz2y_iP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soPqiR1JNmjf"
      },
      "source": [
        "## Saving\n",
        "Save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkoFGCgSZ1yz"
      },
      "outputs": [],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3813cbf2"
      },
      "source": [
        "## Profiling for Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5d933c6"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac5fc4d"
      },
      "source": [
        "Load the tensorboard colab extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74f0c212"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c6131f"
      },
      "source": [
        "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddfd576e"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace/\"\n",
        "\n",
        "def loop_step(batch, step):\n",
        "    input_batch = jnp.array(jnp.array(batch).T)\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "def generate_trace():\n",
        "    tracing_steps = 30\n",
        "    warmup_steps = 5\n",
        "    for current_step in range(warmup_steps + tracing_steps):\n",
        "        if current_step == warmup_steps:\n",
        "            jax.profiler.start_trace(trace_dir)\n",
        "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
        "            batch = next(text_dl)\n",
        "            loop_step(batch, current_step)\n",
        "\n",
        "    jax.profiler.stop_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de70f5b7"
      },
      "source": [
        "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc9452a6"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
        "\n",
        "batch_size = 64\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()\n",
        "\n",
        "batch_size = 256\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea379965"
      },
      "source": [
        "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
        "\n",
        "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
        "\n",
        "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b86c565a"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$trace_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657967a5"
      },
      "source": [
        "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
        "\n",
        "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
        "\n",
        "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
        "\n",
        "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80daa8dc"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "generate_trace()\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
        "generate_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad96e72b"
      },
      "source": [
        "Once again we'll run tensorboard.\n",
        "\n",
        "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
        "\n",
        "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "780e9c72"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$trace_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deca486e"
      },
      "source": [
        "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCApVd7671c1"
      },
      "source": [
        "## Disconnect the Colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsqYdbrDVKSq"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}